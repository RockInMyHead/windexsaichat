import os
import re
import time
from typing import Dict, List, Optional

import requests
from bs4 import BeautifulSoup
from dotenv import load_dotenv

load_dotenv()


class WebSearchEngine:
    """–í–µ–±-–ø–æ–∏—Å–∫–æ–≤–∏–∫ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∞–∫—Ç—É–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞"""

    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update(
            {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
                "Accept-Language": "ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7",
                "Cache-Control": "no-cache",
            }
        )
        
        # Configure proxy if enabled
        proxy_enabled = os.getenv("PROXY_ENABLED", "false").lower() == "true"
        if proxy_enabled:
            proxy_host = os.getenv("PROXY_HOST")
            proxy_port = os.getenv("PROXY_PORT")
            proxy_username = os.getenv("PROXY_USERNAME")
            proxy_password = os.getenv("PROXY_PASSWORD")
            
            if proxy_host and proxy_port:
                if proxy_username and proxy_password:
                    proxy_url = f"http://{proxy_username}:{proxy_password}@{proxy_host}:{proxy_port}"
                else:
                    proxy_url = f"http://{proxy_host}:{proxy_port}"
                
                self.session.proxies = {
                    "http": proxy_url,
                    "https": proxy_url,
                }
                print(f"üåê WebSearchEngine: Proxy enabled {proxy_host}:{proxy_port}")

    def search_google(self, query: str, num_results: int = 5) -> List[Dict[str, str]]:
        """–ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ Google (–±–µ–∑ API)"""
        try:
            # –§–æ—Ä–º–∏—Ä—É–µ–º URL –¥–ª—è –ø–æ–∏—Å–∫–∞
            search_url = f"https://www.google.com/search?q={query}&num={num_results}"

            response = self.session.get(search_url, timeout=10)
            response.raise_for_status()

            soup = BeautifulSoup(response.text, "html.parser")
            results = []

            # –ò—â–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞
            search_results = soup.find_all("div", class_="g")

            for result in search_results[:num_results]:
                try:
                    # –ó–∞–≥–æ–ª–æ–≤–æ–∫ –∏ —Å—Å—ã–ª–∫–∞
                    title_elem = result.find("h3")
                    link_elem = result.find("a")

                    if title_elem and link_elem:
                        title = title_elem.get_text().strip()
                        url = link_elem.get("href", "")

                        # –û—á–∏—â–∞–µ–º URL –æ—Ç Google —Ä–µ–¥–∏—Ä–µ–∫—Ç–æ–≤
                        if url.startswith("/url?q="):
                            url = url.split("/url?q=")[1].split("&")[0]

                        # –û–ø–∏—Å–∞–Ω–∏–µ
                        desc_elem = result.find("span", class_="aCOpRe") or result.find(
                            "div", class_="VwiC3b"
                        )
                        description = desc_elem.get_text().strip() if desc_elem else ""

                        if title and url and url.startswith("http"):
                            results.append(
                                {"title": title, "url": url, "description": description}
                            )
                except Exception as e:
                    continue

            return results

        except Exception as e:
            return []

    def search_duckduckgo(
        self, query: str, num_results: int = 5
    ) -> List[Dict[str, str]]:
        """–ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ DuckDuckGo"""
        try:
            search_url = f"https://html.duckduckgo.com/html/?q={query}"

            response = self.session.get(search_url, timeout=10)
            response.raise_for_status()

            soup = BeautifulSoup(response.text, "html.parser")
            results = []

            # –ò—â–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞
            search_results = soup.find_all("div", class_="result")

            for result in search_results[:num_results]:
                try:
                    title_elem = result.find("a", class_="result__a")
                    desc_elem = result.find("a", class_="result__snippet")

                    if title_elem:
                        title = title_elem.get_text().strip()
                        url = title_elem.get("href", "")
                        description = desc_elem.get_text().strip() if desc_elem else ""

                        if title and url:
                            results.append(
                                {"title": title, "url": url, "description": description}
                            )
                except Exception as e:
                    continue

            return results

        except Exception as e:
            return []

    def search_bing(self, query: str, num_results: int = 5) -> List[Dict[str, str]]:
        """–ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ Bing"""
        try:
            search_url = f"https://www.bing.com/search?q={query}&count={num_results}"

            response = self.session.get(search_url, timeout=10)
            response.raise_for_status()

            soup = BeautifulSoup(response.text, "html.parser")
            results = []

            # –ò—â–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞
            search_results = soup.find_all("li", class_="b_algo")

            for result in search_results[:num_results]:
                try:
                    title_elem = result.find("h2")
                    link_elem = result.find("a")
                    desc_elem = result.find("p")

                    if title_elem and link_elem:
                        title = title_elem.get_text().strip()
                        url = link_elem.get("href", "")
                        description = desc_elem.get_text().strip() if desc_elem else ""

                        if title and url:
                            results.append(
                                {"title": title, "url": url, "description": description}
                            )
                except Exception as e:
                    continue

            return results

        except Exception as e:
            return []

    def search_web(self, query: str, num_results: int = 5) -> List[Dict[str, str]]:
        """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ –≤–µ–±-–∏—Å—Ç–æ—á–Ω–∏–∫–∞–º"""

        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –ø–æ–∏—Å–∫–æ–≤–∏–∫–∏
        search_engines = [self.search_duckduckgo, self.search_bing, self.search_google]

        all_results = []

        for search_func in search_engines:
            try:
                results = search_func(query, num_results)
                if results:
                    all_results.extend(results)
                    break  # –ï—Å–ª–∏ –ø–æ–ª—É—á–∏–ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º—Å—è
            except Exception as e:
                continue

        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ URL
        unique_results = []
        seen_urls = set()

        for result in all_results:
            if result["url"] not in seen_urls:
                unique_results.append(result)
                seen_urls.add(result["url"])

        return unique_results[:num_results]

    def fetch_page_content(self, url: str, max_length: int = 2000) -> str:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        try:

            response = self.session.get(url, timeout=15)
            response.raise_for_status()

            soup = BeautifulSoup(response.text, "html.parser")

            # –£–¥–∞–ª—è–µ–º —Å–∫—Ä–∏–ø—Ç—ã –∏ —Å—Ç–∏–ª–∏
            for script in soup(["script", "style"]):
                script.decompose()

            # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç
            text = soup.get_text()

            # –û—á–∏—â–∞–µ–º —Ç–µ–∫—Å—Ç
            lines = (line.strip() for line in text.splitlines())
            chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
            text = " ".join(chunk for chunk in chunks if chunk)

            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
            if len(text) > max_length:
                text = text[:max_length] + "..."

            return text

        except Exception as e:
            return ""

    def search_and_fetch_content(
        self, query: str, num_results: int = 3
    ) -> List[Dict[str, str]]:
        """–ü–æ–∏—Å–∫ –∏ –ø–æ–ª—É—á–µ–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —Å—Ç—Ä–∞–Ω–∏—Ü"""
        # –°–Ω–∞—á–∞–ª–∞ –∏—â–µ–º
        search_results = self.search_web(query, num_results)

        # –ó–∞—Ç–µ–º –ø–æ–ª—É—á–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
        enriched_results = []

        for result in search_results:
            content = self.fetch_page_content(result["url"])

            enriched_results.append(
                {
                    "title": result["title"],
                    "url": result["url"],
                    "description": result["description"],
                    "content": content,
                }
            )

            # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
            time.sleep(1)

        return enriched_results


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –ø–æ–∏—Å–∫–æ–≤–∏–∫–∞
web_search_engine = WebSearchEngine()


def search_web(query: str, num_results: int = 3) -> List[Dict[str, str]]:
    """–§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ"""
    return web_search_engine.search_and_fetch_content(query, num_results)


def format_search_results(results: List[Dict[str, str]]) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞ –¥–ª—è LLM"""
    if not results:
        return "–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã."

    formatted = "üîç –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ü–û–ò–°–ö–ê –í –ò–ù–¢–ï–†–ù–ï–¢–ï:\n\n"

    for i, result in enumerate(results, 1):
        formatted += f"{i}. **{result['title']}**\n"
        formatted += f"   URL: {result['url']}\n"
        if result["description"]:
            formatted += f"   –û–ø–∏—Å–∞–Ω–∏–µ: {result['description']}\n"
        if result["content"]:
            formatted += f"   –°–æ–¥–µ—Ä–∂–∏–º–æ–µ: {result['content'][:500]}...\n"
        formatted += "\n"

    return formatted
