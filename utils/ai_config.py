"""
AI Configuration - единый конфиг для system-prompt и настроек генерации
"""

# Умный гибкий системный промпт
UNIVERSAL_SYSTEM_PROMPT = {
    "role": "system",
    "content": """Ты — WindexAI, универсальный ИИ-ассистент с широкими возможностями.

Основные принципы:
- Будь полезным, дружелюбным и информативным
- Отвечай на вопросы пользователей четко и по существу
- Для задач предоставляй практические решения и примеры
- Используй Execute-Then-Explain (ETE) подход только для конкретных технических задач

Режимы работы:

1) ДЛЯ ОБЫЧНЫХ ВОПРОСОВ И ОБЩЕНИЯ:
   - Отвечай естественно, как умный собеседник
   - Давай полезные советы и рекомендации
   - Используй эмодзи уместно для дружелюбности

2) ДЛЯ КОНКРЕТНЫХ ЗАДАЧ (программирование, разработка, настройка):
   - Используй Execute-Then-Explain подход
   - Предоставляй готовые решения и артефакты
   - Следуй Output Contract:
     * **Deliverables** — результаты и артефакты
     * **Evidence/Logs** — как проверить/запустить
     * **Notes** — объяснения и замечания
     * **Next** — следующие шаги

Определяй тип запроса автоматически:
- Если пользователь просит совета/рекомендации/объяснения → отвечай естественно
- Если пользователь просит создать/настроить/реализовать → используй ETE с артефактами

Стиль общения:
- Дружелюбный и профессиональный
- Используй markdown для структурирования ответов
- Будь краток, но информативен
- Поддерживай русский язык пользователя

Технические возможности:
- Генерация кода, скриптов, конфигов
- Анализ и решение проблем
- Предоставление примеров и инструкций
"""
}

# Конфигурация моделей (универсальный режим)
MODEL_CONFIGS = {
    "gpt-4o-mini": {
        "name": "WIndexAI Lite",
        "description": "Быстрая и эффективная модель для повседневных задач",
        "max_tokens": 16000,
        "temperature": 0.7,  # оптимально для естественных ответов
        "top_p": 0.9,
        "focus": "универсальный ИИ-ассистент",
        "mode": "universal"
    },
    "gpt-4o": {
        "name": "WIndexAI Pro",
        "description": "Продвинутая модель с расширенными возможностями",
        "max_tokens": 20000,
        "temperature": 0.7,
        "top_p": 0.95,
        "focus": "продвинутый универсальный ИИ-ассистент",
        "mode": "universal"
    }
}

# Единая модель для генерации ответов
DEFAULT_MODEL = "gpt-4o-mini"

def get_model_config(model_name: str) -> dict:
    """Получить конфигурацию модели"""
    return MODEL_CONFIGS.get(model_name, MODEL_CONFIGS["gpt-4o-mini"])

def get_system_prompt() -> dict:
    """Получить универсальный system-prompt"""
    return UNIVERSAL_SYSTEM_PROMPT.copy()

def get_enhanced_user_prompt(user_question: str) -> str:
    """Умная обработка запроса пользователя с учетом контекста."""
    # Проверяем, является ли запрос задачей для выполнения
    task_indicators = [
        'создай', 'сделай', 'напиши', 'реализуй', 'разработай', 'настрой',
        'установи', 'запусти', 'сгенерируй', 'проект', 'приложение', 'система',
        'база данных', 'скрипт', 'код', 'программа', 'инструмент'
    ]

    is_task = any(indicator in user_question.lower() for indicator in task_indicators)

    if is_task:
        # Для задач используем ETE-подход
        return f"""[ЗАДАЧА ДЛЯ ВЫПОЛНЕНИЯ]
{user_question}

[ИНСТРУКЦИЯ]
Используй Execute-Then-Explain подход. Выполни задачу и предоставь готовые артефакты.

[ФОРМАТ ОТВЕТА]
1) **Deliverables** — результаты и артефакты
2) **Evidence/Logs** — проверка и запуск
3) **Notes** — объяснения
4) **Next** — следующие шаги

[СТИЛЬ]
Технический, практический, с готовыми решениями."""
    else:
        # Для обычных вопросов - естественный ответ
        return user_question

def get_generation_params(model: str) -> dict:
    """Получить параметры генерации для модели.

    Всегда используем модель DEFAULT_MODEL для LLM (глобальная замена на gpt5-nano),
    сохраняя прочие параметры в зависимости от выбранного профиля UI.
    """
    config = get_model_config(model)
    return {
        "model": DEFAULT_MODEL,
        "max_tokens": config["max_tokens"],
        "temperature": config["temperature"],
        "top_p": config["top_p"]
    }
