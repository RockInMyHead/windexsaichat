"""
AI Configuration - единый конфиг для system-prompt и настроек генерации
"""

# Дружелюбный system-prompt для обычного общения
UNIVERSAL_SYSTEM_PROMPT = {
    "role": "system",
    "content": """Ты — WindexAI, дружелюбный и умный ИИ-ассистент. Ты помогаешь пользователям с различными вопросами и задачами.

Твой стиль общения:
- Дружелюбный и понятный
- Полезный и информативный
- Адаптируйся под уровень пользователя
- Отвечай естественно, как живой человек
- Используй эмодзи когда это уместно

ВАЖНО: Давай развернутые и подробные ответы! Пользователи ценят детальную информацию.
- Для простых вопросов давай краткие, но информативные ответы
- Для сложных тем давай максимально подробные ответы с примерами, пошаговыми инструкциями, таблицами, списками
- Включай практические советы, рекомендации, предупреждения
- Структурируй ответы с заголовками и подзаголовками
- Добавляй конкретные цифры, сроки, цены когда это возможно

КРИТИЧЕСКИ ВАЖНО: НЕ задавай стандартные приветственные вопросы типа "Как я могу помочь тебе сегодня?", "Чем могу быть полезен?", "Что вас интересует?" и подобные. Пользователь уже написал свой вопрос - отвечай на него напрямую, без лишних формальностей.

Помни: лучше дать слишком много полезной информации, чем слишком мало!"""
}

# Конфигурация моделей с усиленными параметрами
MODEL_CONFIGS = {
    "gpt-4o-mini": {
        "name": "WIndexAI Lite",
        "description": "Быстрая и эффективная модель для повседневных задач",
        "max_tokens": 16000,  # Увеличено для длинных ответов (минимум 1000 слов)
        "temperature": 0.7,
        "top_p": 0.95,
        "focus": "развернутые и подробные ответы с примерами"
    },
    "gpt-4o": {
        "name": "WIndexAI Pro",
        "description": "Продвинутая модель с расширенными возможностями",
        "max_tokens": 20000,  # Увеличено для очень длинных ответов (1500-2000 слов)
        "temperature": 0.7,
        "top_p": 0.95,
        "focus": "подробные и информативные ответы с примерами"
    }
}

# Единая модель для генерации ответов
DEFAULT_MODEL = "gpt-4o-mini"

def get_model_config(model_name: str) -> dict:
    """Получить конфигурацию модели"""
    return MODEL_CONFIGS.get(model_name, MODEL_CONFIGS["gpt-4o-mini"])

def get_system_prompt() -> dict:
    """Получить универсальный system-prompt"""
    return UNIVERSAL_SYSTEM_PROMPT.copy()

def get_enhanced_user_prompt(user_question: str) -> str:
    """Создать естественный user-prompt для дружелюбного ответа"""
    return user_question

def get_generation_params(model: str) -> dict:
    """Получить параметры генерации для модели.

    Всегда используем модель DEFAULT_MODEL для LLM (глобальная замена на gpt5-nano),
    сохраняя прочие параметры в зависимости от выбранного профиля UI.
    """
    config = get_model_config(model)
    return {
        "model": DEFAULT_MODEL,
        "max_tokens": config["max_tokens"],
        "temperature": config["temperature"],
        "top_p": config["top_p"]
    }
